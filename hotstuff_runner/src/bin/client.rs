// ä¿®æ”¹åçš„é«˜æ•ˆå®¢æˆ·ç«¯èŠ‚ç‚¹ - åˆ†ç¦»çŠ¶æ€æ¶æ„
// hotstuff_runner/src/bin/client.rs

use std::collections::HashMap;
use std::net::SocketAddr;
use std::env;
use std::fs::{File, create_dir_all};
use std::time::{Duration, Instant};
use tracing::{info, warn, error};
use tracing_subscriber::{fmt, layer::SubscriberExt, util::SubscriberInitExt};
use std::thread;
use std::fs;
use ed25519_dalek::SigningKey;
use serde::{Serialize, Deserialize};
use tokio::net::TcpStream;
use tokio::io::{AsyncWriteExt, AsyncReadExt};
use rand::Rng;
use std::sync::Arc;
use std::collections::HashSet;

#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct TestTransaction {
    pub id: u64,
    pub from: String,
    pub to: String,
    pub amount: u64,
    pub timestamp: u64,
    pub nonce: u64,
}

#[derive(Serialize, Deserialize, Debug)]
pub struct ClientMessage {
    pub message_type: String,
    pub transaction: Option<TestTransaction>,
    pub client_id: String,
}

// åˆ†ç¦»çŠ¶æ€ï¼šæ ¸å¿ƒä¸šåŠ¡é€»è¾‘ï¼ˆæ— éœ€å…±äº«ï¼‰
pub struct ClientNode {
    client_id: String,
    connections: HashMap<usize, PersistentConnection>,
    tx_generator: TransactionGenerator,
    stats: ClientStats,
    response_tx: Option<tokio::sync::mpsc::UnboundedSender<ResponseCommand>>, 
}

// åˆ†ç¦»çŠ¶æ€ï¼šå»¶è¿Ÿè·Ÿè¸ªå™¨ï¼ˆç‹¬ç«‹è¿è¡Œï¼‰
pub struct LatencyTracker {
    send_timestamps: HashMap<u64, Instant>,
    ordering_latencies: Vec<u128>,
    consensus_latencies: Vec<u128>,
    ordering_recorded: HashSet<u64>, 
    consensus_recorded: HashSet<u64>
}

// åˆ†ç¦»çŠ¶æ€ï¼šç»Ÿè®¡æŠ¥å‘Šå™¨ï¼ˆç‹¬ç«‹è¿è¡Œï¼‰
pub struct StatsReporter {
    total_responses: usize,
}

// å“åº”å‘½ä»¤æšä¸¾ï¼šå»¶è¿Ÿè·Ÿè¸ªå™¨å¤„ç†å“åº”
#[derive(Serialize, Deserialize, Debug, Clone)]
pub enum ResponseCommand {
    Ordering1Response { tx_ids: Vec<u64> },
    HotStuffCommitted { tx_ids: Vec<u64> },
    Error { tx_ids: Vec<u64>, error_msg: String },
}

// #[derive(Serialize, Deserialize, Debug, Clone)]
// pub enum ResponseMessageContent {
//     Ordering1Response {
//         tx_id: u64,
//         timestamp_us: u64,
//         node_id: usize,
//     },
//     HotStuffCommitted {
//         tx_id: u64,
//         timestamp_us: u64,
//         node_id: usize,
//     },
//     Error {
//         tx_id: u64,
//         error_msg: String,
//     },
// }

// #[derive(Serialize, Deserialize, Debug, Clone)]
// pub struct ResponseMessage {
//     pub message_type: String,
//     pub response: Option<ResponseMessageContent>,
//     pub node_id: usize,
// }

impl LatencyTracker {
    pub fn new() -> Self {
        Self {
            send_timestamps: HashMap::new(),
            ordering_latencies: Vec::new(),
            consensus_latencies: Vec::new(),
            ordering_recorded: HashSet::new(),
            consensus_recorded: HashSet::new(),
        }
    }

    pub fn record_send_time(&mut self, tx_id: u64) {
        self.send_timestamps.insert(tx_id, Instant::now());
    }

    // ğŸ”¥ ä¿®æ”¹ï¼šæ”¯æŒæ‰¹é‡å¤„ç† ordering å“åº”
    pub fn handle_ordering_response(&mut self, tx_ids: Vec<u64>) {
        for tx_id in tx_ids {
            // åªè®°å½•ç¬¬ä¸€æ¬¡
            if self.ordering_recorded.contains(&tx_id) {
                continue;
            }
            if let Some(send_time) = self.send_timestamps.get(&tx_id) {
                let latency = send_time.elapsed().as_micros();
                let latency_ms=latency as f64 / 1000.0;
                self.ordering_latencies.push(latency);
                self.ordering_recorded.insert(tx_id);
                info!("ğŸ“Š äº¤æ˜“ {} orderingå»¶è¿Ÿ: {}ms", tx_id, latency_ms);
            }
        }
    }

    // ğŸ”¥ ä¿®æ”¹ï¼šæ”¯æŒæ‰¹é‡å¤„ç† consensus å“åº”
    pub fn handle_consensus_response(&mut self, tx_ids: Vec<u64>) {
        for tx_id in tx_ids {
            if self.consensus_recorded.contains(&tx_id) {
                continue;
            }
            if let Some(send_time) = self.send_timestamps.remove(&tx_id) {
                let latency = send_time.elapsed().as_micros();
                let latency_ms=latency as f64 / 1000.0;
                self.consensus_latencies.push(latency);
                self.consensus_recorded.insert(tx_id);
                info!("ğŸ“Š äº¤æ˜“ {} consensuså»¶è¿Ÿ: {}ms", tx_id, latency_ms);
            }
        }
    }

    pub fn get_stats(&self) -> (usize, usize) {
        (self.ordering_latencies.len(), self.consensus_latencies.len())
    }

    pub fn print_ordering_stats(&self) {
        if self.ordering_latencies.is_empty() { return; }
        
        let mut sorted = self.ordering_latencies.clone();
        sorted.sort();
        
        let avg = sorted.iter().sum::<u128>() as f64 / sorted.len() as f64;
        let p50 = sorted[sorted.len() / 2];
        let p95 = sorted[sorted.len() * 95 / 100];
        let p99 = sorted[sorted.len() * 99 / 100];
        
        info!("ğŸ“ˆ Orderingå»¶è¿Ÿç»Ÿè®¡ (æ ·æœ¬: {}):", sorted.len());
        info!("  å¹³å‡å€¼: {:.2} ms", avg as f64 / 1000.0);
        info!("  P50: {} ms", p50 as f64 / 1000.0);
        info!("  P95: {} ms", p95 as f64 / 1000.0);
        info!("  P99: {} ms", p99 as f64 / 1000.0);
    }

    pub fn print_consensus_stats(&self) {
        if self.consensus_latencies.is_empty() { return; }
        
        let mut sorted = self.consensus_latencies.clone();
        sorted.sort();
        
        let avg = sorted.iter().sum::<u128>() as f64 / sorted.len() as f64;
        let p50 = sorted[sorted.len() / 2];
        let p95 = sorted[sorted.len() * 95 / 100];
        let p99 = sorted[sorted.len() * 99 / 100];
        
        info!("ğŸ“ˆ Consensuså»¶è¿Ÿç»Ÿè®¡ (æ ·æœ¬: {}):", sorted.len());
        info!("  å¹³å‡å€¼: {:.2} ms", avg as f64 / 1000.0);
        info!("  P50: {} ms", p50 as f64 / 1000.0);
        info!("  P95: {} ms", p95 as f64 / 1000.0);
        info!("  P99: {} ms", p99 as f64 / 1000.0);
    }

    pub fn print_comprehensive_stats(&self) {
        info!("ğŸ“Š ============= ç»¼åˆå»¶è¿Ÿç»Ÿè®¡æŠ¥å‘Š =============");
        self.print_ordering_stats();
        self.print_consensus_stats();
        
        if !self.ordering_latencies.is_empty() && !self.consensus_latencies.is_empty() {
            let avg_ordering_ms = self.ordering_latencies.iter().sum::<u128>() as f64 / self.ordering_latencies.len() as f64 / 1000.0;
            let avg_consensus_ms = self.consensus_latencies.iter().sum::<u128>() as f64 / self.consensus_latencies.len() as f64 / 1000.0;
            
            info!("ğŸ“Š å»¶è¿Ÿå¯¹æ¯”åˆ†æ:");
            info!("  Orderingå¹³å‡å»¶è¿Ÿ: {:.2} ms", avg_ordering_ms);
            info!("  Consensuså¹³å‡å»¶è¿Ÿ: {:.2} ms", avg_consensus_ms);
            info!("  Consensus/Orderingæ¯”å€¼: {:.2}x", avg_consensus_ms / avg_ordering_ms);
        }
        info!("ğŸ“Š ==========================================");
    }

    pub fn save_latency_data(&self, ordering_file: &str, consensus_file: &str) -> Result<(), Box<dyn std::error::Error>> {
        use std::io::Write;
        
        if !self.ordering_latencies.is_empty() {
            let mut file = File::create(ordering_file)?;
            writeln!(file, "latency_us")?;
            for latency in &self.ordering_latencies {
                writeln!(file, "{}", latency)?;
            }
            info!("ğŸ’¾ Orderingå»¶è¿Ÿæ•°æ®å·²ä¿å­˜åˆ°: {}", ordering_file);
        }

        if !self.consensus_latencies.is_empty() {
            let mut file = File::create(consensus_file)?;
            writeln!(file, "latency_us")?;
            for latency in &self.consensus_latencies {
                writeln!(file, "{}", latency)?;
            }
            info!("ğŸ’¾ Consensuså»¶è¿Ÿæ•°æ®å·²ä¿å­˜åˆ°: {}", consensus_file);
        }

        Ok(())
    }
}

impl StatsReporter {
    pub fn new() -> Self {
        Self { total_responses: 0 }
    }

    pub fn record_response(&mut self) {
        self.total_responses += 1;
    }

    pub fn should_print_stats(&self) -> bool {
        self.total_responses > 0 && self.total_responses % 100 == 0
    }
}

// å‘½ä»¤æšä¸¾ï¼šä¸šåŠ¡é€»è¾‘ä¸å»¶è¿Ÿè·Ÿè¸ªé€šä¿¡
#[derive(Debug)]
pub enum ClientCommand {
    SendBatch {
        node_id: usize,
        transactions: Vec<TestTransaction>,
        reply_tx: tokio::sync::oneshot::Sender<Result<usize, Box<dyn std::error::Error + Send>>>,
    },
    RecordSendTimes {
        tx_ids: Vec<u64>,
    },
    PrintStats,
    GetConnectionCount {
        reply_tx: tokio::sync::oneshot::Sender<usize>,
    },
}


impl ClientNode {
    pub fn new(client_id: String) -> Self {
        info!("ğŸš€ åˆå§‹åŒ–å®¢æˆ·ç«¯æ ¸å¿ƒ: {}", client_id);
        
        let tx_generator = TransactionGenerator::new(client_id.clone());

        Self {
            client_id,
            connections: HashMap::new(),
            tx_generator,
            stats: ClientStats::default(),
            response_tx: None,
        }
    }
    pub fn set_response_sender(&mut self, response_tx: tokio::sync::mpsc::UnboundedSender<ResponseCommand>) {
        self.response_tx = Some(response_tx);
    }

    pub async fn establish_connections(
        &mut self, 
        node_least_id: usize, 
        node_num: usize,
        response_tx: tokio::sync::mpsc::UnboundedSender<ResponseCommand>,
    ) -> Result<(), Box<dyn std::error::Error>> {
        info!("ğŸŒ å»ºç«‹åˆ°æ‰€æœ‰èŠ‚ç‚¹çš„æŒä¹…è¿æ¥...");
        self.response_tx = Some(response_tx.clone());
         

        for node_id in node_least_id..(node_least_id + node_num) {
            match PersistentConnection::new(node_id,response_tx.clone()).await {
                Ok(conn) => {
                    self.connections.insert(node_id, conn);
                    info!("âœ… è¿æ¥åˆ°èŠ‚ç‚¹ {} æˆåŠŸ", node_id);
                }
                Err(e) => {
                    error!("âŒ è¿æ¥åˆ°èŠ‚ç‚¹ {} å¤±è´¥: {}", node_id, e);
                }
            }
        }

        info!("ğŸ¯ æˆåŠŸå»ºç«‹ {} ä¸ªæŒä¹…è¿æ¥", self.connections.len());
        Ok(())
    }

    pub async fn send_batch_to_node(
        &mut self, 
        node_id: usize, 
        transactions: Vec<TestTransaction>,
    ) -> Result<usize, Box<dyn std::error::Error>> {
        if let Some(connection) = self.connections.get_mut(&node_id) {
            match connection.send_batch(&transactions, &self.client_id).await {
                Ok(sent_count) => {
                    self.stats.record_sent(sent_count as u64);
                    self.stats.record_confirmed(sent_count as u64);
                    Ok(sent_count)
                }
                Err(e) => {
                    error!("âŒ æ‰¹é‡å‘é€åˆ°èŠ‚ç‚¹ {} å¤±è´¥: {}", node_id, e);
                    self.stats.record_failed(transactions.len() as u64);
                    
                    // å°è¯•é‡æ–°è¿æ¥
                    if let Some(response_tx) = &self.response_tx {
                    info!("ğŸ”„ å°è¯•é‡æ–°è¿æ¥åˆ°èŠ‚ç‚¹ {}", node_id);
                        match PersistentConnection::new(node_id,response_tx.clone()).await {
                            Ok(new_conn) => {
                                self.connections.insert(node_id, new_conn);
                                info!("âœ… é‡æ–°è¿æ¥åˆ°èŠ‚ç‚¹ {} æˆåŠŸ", node_id);
                            }
                            Err(reconnect_err) => {
                                error!("âŒ é‡æ–°è¿æ¥åˆ°èŠ‚ç‚¹ {} å¤±è´¥: {}", node_id, reconnect_err);
                            }
                        }
                    }
                    Err(e)
                }
            }
        } else {
            error!("âŒ æ²¡æœ‰åˆ°èŠ‚ç‚¹ {} çš„è¿æ¥", node_id);
            Err("æ²¡æœ‰è¿æ¥".into())
        }
    }

    pub fn get_connection_count(&self) -> usize {
        self.connections.len()
    }

    pub async fn run_load_test(
        &mut self, 
        config: LoadTestConfig, 
        node_least_id: usize, 
        node_num: usize, 
        cmd_tx: tokio::sync::mpsc::UnboundedSender<ClientCommand>,
    ) {
        info!("ğŸš€ å¼€å§‹è´Ÿè½½æµ‹è¯• - TPSç›®æ ‡: {}, æŒç»­æ—¶é—´: {}ç§’", 
            config.target_tps, config.duration_secs);

        let batch_size = std::cmp::max(100, config.target_tps / 5);
        let batch_interval = Duration::from_millis(200);
        let end_time = Instant::now() + Duration::from_secs(config.duration_secs);

        let mut total_sent = 0;
        let mut batch_counter = 0;

        while Instant::now() < end_time {
            for node_offset in 0..node_num {
                let node_id = node_least_id + node_offset;
                let transactions = self.tx_generator.generate_batch(batch_size as usize);
                
                // å…ˆé€šçŸ¥å»¶è¿Ÿè·Ÿè¸ªå™¨è®°å½•å‘é€æ—¶é—´
                let tx_ids: Vec<u64> = transactions.iter().map(|tx| tx.id).collect();
                let _ = cmd_tx.send(ClientCommand::RecordSendTimes { tx_ids });

                match self.send_batch_to_node(node_id, transactions).await {
                    Ok(sent_count) => {
                        total_sent += sent_count;
                        info!("ğŸ“¦ æ‰¹æ¬¡ {} å‘é€ {} ä¸ªäº¤æ˜“åˆ°èŠ‚ç‚¹ {}", batch_counter + 1, sent_count, node_id);
                    }
                    Err(e) => {
                        warn!("æ‰¹æ¬¡ {} å‘é€åˆ°èŠ‚ç‚¹ {} å¤±è´¥: {}", batch_counter + 1, node_id, e);
                    }
                }
            }

            batch_counter += 1;

            if total_sent >= 5000 && total_sent % 5000 == 0 {
                self.stats.log_summary();
            }

            tokio::time::sleep(batch_interval).await;
        }

        info!("ğŸ è´Ÿè½½æµ‹è¯•å®Œæˆï¼Œæ€»è®¡å‘é€ {} ä¸ªäº¤æ˜“", total_sent);
        self.stats.log_summary();
    }

    pub async fn run_interactive_mode(
        &mut self, 
        node_least_id: usize, 
        node_num: usize, 
        cmd_tx: tokio::sync::mpsc::UnboundedSender<ClientCommand>, 
    ) {
        info!("ğŸ® è¿›å…¥äº¤äº’æ¨¡å¼");

        let mut tx_counter = 0;
        
        loop {
            let batch_size = 5;
            let transactions = self.tx_generator.generate_batch(batch_size);
            let target_node = (tx_counter / batch_size) % node_num + node_least_id;

            // å…ˆé€šçŸ¥å»¶è¿Ÿè·Ÿè¸ªå™¨è®°å½•å‘é€æ—¶é—´
            let tx_ids: Vec<u64> = transactions.iter().map(|tx| tx.id).collect();
            let _ = cmd_tx.send(ClientCommand::RecordSendTimes { tx_ids });

            match self.send_batch_to_node(target_node, transactions).await {
                Ok(sent_count) => {
                    tx_counter += sent_count;
                    info!("âœ… æˆåŠŸå‘é€ {} ä¸ªäº¤æ˜“åˆ°èŠ‚ç‚¹ {}, æ€»è®¡: {}", sent_count, target_node, tx_counter);
                }
                Err(e) => {
                    error!("âŒ å‘é€æ‰¹æ¬¡å¤±è´¥åˆ°èŠ‚ç‚¹ {}: {}", target_node, e);
                    tokio::time::sleep(Duration::from_secs(5)).await;
                    continue;
                }
            }

            if tx_counter >= 100 && tx_counter % 100 == 0 {
                self.stats.log_summary();
            }

            tokio::time::sleep(Duration::from_millis(1000)).await;
        }
    }
}

// å…¶ä»–ç»“æ„ä½“ä¿æŒä¸å˜...
pub struct TransactionGenerator {
    current_tx_id: u64,
    current_nonce: u64,
    client_id: String,
    accounts: Vec<String>,
}

impl TransactionGenerator {
    pub fn new(client_id: String) -> Self {
        let accounts = vec![
            "alice".to_string(),
            "bob".to_string(),
            "charlie".to_string(),
            "david".to_string(),
            "eve".to_string(),
        ];

        Self {
            current_tx_id: 0,
            current_nonce: 0,
            client_id,
            accounts,
        }
    }

    pub fn generate_transaction(&mut self) -> TestTransaction {
        let mut rng = rand::thread_rng();
        
        let from_idx = rng.gen_range(0, self.accounts.len());
        let mut to_idx = rng.gen_range(0, self.accounts.len());
        while to_idx == from_idx {
            to_idx = rng.gen_range(0, self.accounts.len());
        }

        let from = self.accounts[from_idx].clone();
        let to = self.accounts[to_idx].clone();
        let amount = rng.gen_range(1, 100000);

        self.current_tx_id += 1;
        self.current_nonce += 1;

        TestTransaction {
            id: self.current_tx_id,
            from,
            to,
            amount,
            timestamp: std::time::SystemTime::now()
                .duration_since(std::time::UNIX_EPOCH)
                .unwrap()
                .as_secs(),
            nonce: self.current_nonce,
        }
    }

    pub fn generate_batch(&mut self, count: usize) -> Vec<TestTransaction> {
        (0..count).map(|_| self.generate_transaction()).collect()
    }
}

#[derive(Default)]
pub struct ClientStats {
    pub total_sent: u64,
    pub total_confirmed: u64,
    pub total_failed: u64,
    pub start_time: Option<Instant>,
}

impl ClientStats {
    pub fn record_sent(&mut self, count: u64) {
        if self.start_time.is_none() {
            self.start_time = Some(Instant::now());
        }
        self.total_sent += count;
    }

    pub fn record_confirmed(&mut self, count: u64) {
        self.total_confirmed += count;
    }

    pub fn record_failed(&mut self, count: u64) {
        self.total_failed += count;
    }

    pub fn calculate_tps(&self) -> f64 {
        if let Some(start_time) = self.start_time {
            let elapsed = start_time.elapsed().as_secs_f64();
            if elapsed > 0.0 {
                return self.total_sent as f64 / elapsed;
            }
        }
        0.0
    }

    pub fn log_summary(&self) {
        let tps = self.calculate_tps();
        let success_rate = if self.total_sent > 0 {
            (self.total_confirmed as f64 / self.total_sent as f64) * 100.0
        } else {
            0.0
        };

        info!(
            "ğŸ“Š å®¢æˆ·ç«¯ç»Ÿè®¡ - å‘é€: {}, ç¡®è®¤: {}, å¤±è´¥: {}, TPS: {:.2}, æˆåŠŸç‡: {:.1}%",
            self.total_sent,
            self.total_confirmed,
            self.total_failed,
            tps,
            success_rate
        );
    }
}

pub struct PersistentConnection {
    // stream: TcpStream,
    write_stream: tokio::net::tcp::OwnedWriteHalf, // ğŸ”¥ åªä¿å­˜å†™æµ
    node_id: usize,
    connected_at: Instant,
}

impl PersistentConnection {
    pub async fn new(
        node_id: usize,
        response_tx: tokio::sync::mpsc::UnboundedSender<ResponseCommand>,
    ) -> Result<Self, Box<dyn std::error::Error>> {
        let hostname = format!("node{}", node_id);
        let port = 9000 + node_id as u16;
        let addr_str = format!("{}:{}", hostname, port);

        info!("ğŸ”— å»ºç«‹æŒä¹…è¿æ¥åˆ°èŠ‚ç‚¹ {}: {}", node_id, addr_str);

        let stream = TcpStream::connect(&addr_str).await?;
    
        // ğŸ”¥ å…³é”®ï¼šåˆ†ç¦»è¯»å†™æµ
        let (read_half, write_half) = stream.into_split();

        tokio::spawn(async move {
            if let Err(e) = handle_node_responses(node_id, read_half, response_tx).await {
                error!("âŒ èŠ‚ç‚¹ {} å“åº”æ¥æ”¶å¤±è´¥: {}", node_id, e);
            }
        });
        info!("âœ… æˆåŠŸå»ºç«‹æŒä¹…è¿æ¥åˆ°èŠ‚ç‚¹ {}", node_id);

        Ok(Self {
            write_stream: write_half,
            node_id,
            connected_at: Instant::now(),
        })
    }

    pub async fn send_batch(&mut self, transactions: &[TestTransaction], client_id: &str) -> Result<usize, Box<dyn std::error::Error>> {
        let mut batch_buffer = Vec::new();

        let is_pompe = true; /////// è°ƒè¯•ä¿®æ”¹ç‚¹

        if is_pompe {
            for transaction in transactions {
                let client_message = ClientMessage {
                    message_type: "pompe_transaction".to_string(),
                    transaction: Some(transaction.clone()),
                    client_id: client_id.to_string(),
                };
            
                let serialized = serde_json::to_vec(&client_message)?;
                let message_length = serialized.len() as u32;
                info!("ğŸ“¦ ******* å®¢æˆ·ç«¯å‘é€æ¶ˆæ¯ï¼Œé•¿åº¦: {} bytes", message_length);

                batch_buffer.extend_from_slice(&message_length.to_be_bytes());
                batch_buffer.extend_from_slice(&serialized);
            }
        } else {
            for transaction in transactions {
                let client_message = ClientMessage {
                    message_type: "transaction".to_string(),
                    transaction: Some(transaction.clone()),
                    client_id: client_id.to_string(),
                };
            
                let serialized = serde_json::to_vec(&client_message)?;
                let message_length = serialized.len() as u32;
                info!("ğŸ“¦ ******* å®¢æˆ·ç«¯å‘é€æ¶ˆæ¯ï¼Œé•¿åº¦: {} bytes", message_length);

                batch_buffer.extend_from_slice(&message_length.to_be_bytes());
                batch_buffer.extend_from_slice(&serialized);
            }
        }

        self.write_stream.write_all(&batch_buffer).await?;
        self.write_stream.flush().await?;

        Ok(transactions.len())
    }

    pub fn uptime(&self) -> Duration {
        self.connected_at.elapsed()
    }
}

pub struct LoadTestConfig {
    pub target_tps: u32,
    pub duration_secs: u64,
}

fn setup_tracing_logger(mode: &str) {
    create_dir_all("logs").expect("æ— æ³•åˆ›å»ºæ—¥å¿—ç›®å½•");

    let path = match mode {
        "interactive" => "client".to_string(),
        "load_test" => "load_test".to_string(),
        _ => {
            warn!("âš ï¸ æœªçŸ¥æ¨¡å¼ï¼Œä½¿ç”¨é»˜è®¤æ—¥å¿—é…ç½®");
            "default".to_string()
        }
    };

    let _ = fs::remove_file(format!("logs/{}.log", path));

    let log_file = File::options()
        .create(true)
        .append(true)
        .open(format!("logs/{}.log", path))
        .expect("æ— æ³•æ‰“å¼€æ—¥å¿—æ–‡ä»¶");
    
    let result = tracing_subscriber::registry()
        .with(
            fmt::layer()
                .with_writer(std::io::stdout)
                .with_target(true)
                .with_thread_ids(true)
                .with_ansi(true)
        )
        .with(
            fmt::layer()
                .with_writer(log_file)
                .with_target(true)
                .with_thread_ids(true)
                .with_ansi(false)
        )
        .try_init();
    
    match result {
        Ok(_) => info!("ğŸ“ å®¢æˆ·ç«¯æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ"),
        Err(_) => warn!("âš ï¸ æ—¥å¿—ç³»ç»Ÿå·²ç»åˆå§‹åŒ–è¿‡äº†ï¼Œè·³è¿‡"),
    }
}

// ğŸ”¥ ä¿®æ”¹ç½‘ç»œå“åº”è§£æï¼Œæ”¯æŒæ‰¹é‡æ¶ˆæ¯
async fn handle_node_responses(
    node_id: usize,
    mut read_half: tokio::net::tcp::OwnedReadHalf,
    response_tx: tokio::sync::mpsc::UnboundedSender<ResponseCommand>,
) -> Result<(), Box<dyn std::error::Error>> {
    let mut length_buf = [0u8; 4];
    
    info!("ğŸ§ å¯åŠ¨èŠ‚ç‚¹ {} çš„å“åº”æ¥æ”¶å™¨", node_id);
    
    loop {
        match read_half.read_exact(&mut length_buf).await {
            Ok(_) => {
                let message_length = u32::from_be_bytes(length_buf) as usize;
                
                if message_length > 1024 * 1024 {
                    warn!("âš ï¸ ä»èŠ‚ç‚¹ {} æ”¶åˆ°è¿‡å¤§å“åº”: {}", node_id, message_length);
                    continue;
                }
                
                let mut message_buf = vec![0u8; message_length];
                read_half.read_exact(&mut message_buf).await?;
                
                // ğŸ”¥ è§£æå“åº”æ¶ˆæ¯ï¼Œæ”¯æŒæ‰¹é‡ tx_ids
                if let Ok(response_json) = serde_json::from_slice::<serde_json::Value>(&message_buf) {
                    if let Some(message_type) = response_json.get("message_type").and_then(|v| v.as_str()) {
                        
                        // ğŸ”¥ æ”¯æŒå•ä¸ª tx_id æˆ– tx_ids æ•°ç»„
                        let tx_ids = if let Some(tx_ids_array) = response_json.get("tx_ids") {
                            // æ‰¹é‡äº¤æ˜“ ID
                            serde_json::from_value::<Vec<u64>>(tx_ids_array.clone())
                                .unwrap_or_else(|_| Vec::new())
                        } else if let Some(tx_id) = response_json.get("tx_id").and_then(|v| v.as_u64()) {
                            // å•ä¸ªäº¤æ˜“ IDï¼ˆå‘åå…¼å®¹ï¼‰
                            vec![tx_id]
                        } else {
                            warn!("âš ï¸ å“åº”æ¶ˆæ¯ä¸­æ²¡æœ‰ tx_id æˆ– tx_ids");
                            continue;
                        };
                        
                        if tx_ids.is_empty() {
                            warn!("âš ï¸ å“åº”æ¶ˆæ¯ä¸­ tx_ids ä¸ºç©º");
                            continue;
                        }
                        
                        let tx_ids_len = tx_ids.len(); // Store length before moving tx_ids
                        
                        let response_cmd = match message_type {
                            "pompe_ordering1_response" => {
                                ResponseCommand::Ordering1Response { tx_ids }
                            }
                            "consensus_response" => {
                                ResponseCommand::HotStuffCommitted { tx_ids }
                            }
                            "error_response" => {
                                let error_msg = response_json.get("error_msg")
                                    .and_then(|v| v.as_str())
                                    .unwrap_or("æœªçŸ¥é”™è¯¯")
                                    .to_string();
                                ResponseCommand::Error { tx_ids, error_msg }
                            }
                            _ => {
                                warn!("âš ï¸ æœªçŸ¥å“åº”ç±»å‹: {}", message_type);
                                continue;
                            }
                        };
                        
                        // å‘é€æ‰¹é‡å“åº”å‘½ä»¤
                        let _ = response_tx.send(response_cmd);
                        info!("âœ… ä»èŠ‚ç‚¹ {} å¤„ç†æ‰¹é‡å“åº”: {} {} ä¸ªäº¤æ˜“", 
                              node_id, message_type, tx_ids_len);
                    }
                } else {
                    warn!("âš ï¸ æ— æ³•è§£æä»èŠ‚ç‚¹ {} æ”¶åˆ°çš„å“åº”", node_id);
                }
            }
            Err(e) => {
                info!("ğŸ”Œ èŠ‚ç‚¹ {} è¿æ¥æ–­å¼€: {}", node_id, e);
                break;
            }
        }
    }
    
    Ok(())
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let client_id = env::var("CLIENT_ID").unwrap_or_else(|_| "client_1".to_string());
    let mode = env::var("CLIENT_MODE").unwrap_or_else(|_| "interactive".to_string());
    setup_tracing_logger(mode.as_str());
    
    let node_least_id: usize = env::var("NODE_LEAST_ID")
        .unwrap_or_else(|_| "0".to_string())
        .parse()
        .expect("NODE_LEAST_ID å¿…é¡»æ˜¯æ•°å­—");
    let node_num: usize = env::var("NODE_NUM")
        .unwrap_or_else(|_| "4".to_string())
        .parse()
        .expect("NODE_NUM å¿…é¡»æ˜¯æ•°å­—");

    info!("ğŸƒ å¯åŠ¨åˆ†ç¦»çŠ¶æ€å®¢æˆ·ç«¯: {}", client_id);

    // åˆ›å»ºé€šé“
    let (cmd_tx, mut cmd_rx) = tokio::sync::mpsc::unbounded_channel::<ClientCommand>();
    let (response_tx, mut response_rx) = tokio::sync::mpsc::unbounded_channel::<ResponseCommand>();

    // å¯åŠ¨å»¶è¿Ÿè·Ÿè¸ªå™¨ä»»åŠ¡
    let latency_cmd_tx = cmd_tx.clone();
    tokio::spawn(async move {
        let mut latency_tracker = LatencyTracker::new();
        let mut stats_reporter = StatsReporter::new();

        loop {
            tokio::select! {
                Some(cmd) = cmd_rx.recv() => {
                    match cmd {
                        ClientCommand::RecordSendTimes { tx_ids } => {
                            for tx_id in tx_ids {
                                latency_tracker.record_send_time(tx_id);
                            }
                        }
                        ClientCommand::PrintStats => {
                            latency_tracker.print_comprehensive_stats();
                        }
                        _ => {} // å…¶ä»–å‘½ä»¤ç”±ä¸»ä»»åŠ¡å¤„ç†
                    }
                }
                Some(response_cmd) = response_rx.recv() => {
                    match response_cmd {
                        // ğŸ”¥ ä¿®æ”¹ï¼šå¤„ç†æ‰¹é‡ ordering å“åº”
                        ResponseCommand::Ordering1Response { tx_ids } => {
                            info!("ğŸ‰ æ”¶åˆ° {} ä¸ª Ordering1 å“åº” for {:?}", tx_ids.len(), tx_ids);
                            latency_tracker.handle_ordering_response(tx_ids);
                        }
                        // ğŸ”¥ ä¿®æ”¹ï¼šå¤„ç†æ‰¹é‡ consensus å“åº”
                        ResponseCommand::HotStuffCommitted { tx_ids } => { 
                            info!("ğŸ‰ æ”¶åˆ° {} ä¸ª Consensus å“åº”", tx_ids.len());
                            latency_tracker.handle_consensus_response(tx_ids);
                        }
                        ResponseCommand::Error { tx_ids, error_msg } => {
                            error!("âŒ {} ä¸ªäº¤æ˜“å¤„ç†å¤±è´¥: {}", tx_ids.len(), error_msg);
                            for tx_id in tx_ids {
                                error!("âŒ äº¤æ˜“ {} å¤±è´¥", tx_id);
                            }
                        }
                    }
                    
                    stats_reporter.record_response();
                    if stats_reporter.should_print_stats() {
                        latency_tracker.print_comprehensive_stats();
                    }
                }
            }
        }
    });

    // åˆ›å»ºå¹¶å¯åŠ¨å®¢æˆ·ç«¯æ ¸å¿ƒ
    let mut client_core = ClientNode::new(client_id);

    // ç­‰å¾…å…±è¯†èŠ‚ç‚¹å¯åŠ¨
    info!("â³ ç­‰å¾…å…±è¯†èŠ‚ç‚¹å¯åŠ¨...");
    tokio::time::sleep(Duration::from_secs(15)).await;

    // å»ºç«‹è¿æ¥
    if let Err(e) = client_core.establish_connections(node_least_id, node_num, response_tx.clone()).await {
        error!("âŒ å»ºç«‹è¿æ¥å¤±è´¥: {}", e);
        return Err(e);
    }

    // è¿è¡Œä¸»è¦é€»è¾‘
    match mode.as_str() {
        "load_test" => {
            let target_tps: u32 = env::var("TARGET_TPS")
                .unwrap_or_else(|_| "100".to_string())
                .parse()
                .unwrap_or(100);
            
            let duration: u64 = env::var("TEST_DURATION")
                .unwrap_or_else(|_| "60".to_string())
                .parse()
                .unwrap_or(60);

            let config = LoadTestConfig {
                target_tps,
                duration_secs: duration,
            };

            client_core.run_load_test(config, node_least_id, node_num, cmd_tx.clone()).await;

            info!("âœ… è´Ÿè½½æµ‹è¯•å®Œæˆï¼Œç­‰å¾…å“åº”å¤„ç†...");
            tokio::time::sleep(Duration::from_secs(30)).await;
            
            // è¯·æ±‚æ‰“å°æœ€ç»ˆæŠ¥å‘Š
            let _ = cmd_tx.send(ClientCommand::PrintStats);
            tokio::time::sleep(Duration::from_secs(2)).await;
        }
        _ => {
            client_core.run_interactive_mode(node_least_id, node_num, cmd_tx.clone()).await;
        }
    }

    Ok(())
}